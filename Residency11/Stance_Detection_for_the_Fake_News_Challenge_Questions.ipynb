{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Stance_Detection_for_the_Fake_News_Challenge_Questions.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QI9jhXKPCFcJ"},"source":["# Stance Detection for the Fake News Challenge\n","\n","## Identifying Textual Relationships with Deep Neural Nets\n","\n","### Check the problem context [here](https://drive.google.com/open?id=1KfWaZyQdGBw8AUTacJ2yY86Yxgw2Xwq0).\n","\n","### Download files required for the project from [here](https://drive.google.com/open?id=10yf39ifEwVihw4xeJJR60oeFBY30Y5J8)."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vSNgdEMpenpE"},"source":["## Step1: Load the given dataset  \n","\n","1. Mount the google drive\n","\n","2. Import Glove embeddings\n","\n","3. Import the test and train datasets"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aPOZRohMiSpQ"},"source":["### Mount the google drive to access required project files\n","\n","Run the below commands"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7AS39z1XgFpT","outputId":"96ca9e38-59f8-4d27-f06d-5a3d2a8fab80","executionInfo":{"status":"ok","timestamp":1579950096623,"user_tz":-330,"elapsed":2088,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bhZdJ4zpwWzN"},"source":["#### Path for Project files on google drive\n","\n","**Note:** You need to change this path according where you have kept the files in google drive. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Aol97RUogFuS","colab":{}},"source":["project_path = \"/content/drive/My Drive/Fake News Challenge/\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2ly0VxAnwJ2f"},"source":["### Loading the Glove Embeddings\n","The smallest package of embeddings is 822Mb, called “glove.6B.zip“. It was trained on a dataset of one billion tokens (words) with a vocabulary of 400 thousand words. There are a few different embedding vector sizes, including 50, 100, 200 and 300 dimensions.\n","\n","\"glove.6B.zip\" is already provided to us; we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your training dataset.\n","\n","[Relevant article](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xmsPn6PF-cgL","colab":{}},"source":["from zipfile import ZipFile\n","with ZipFile(project_path + 'glove.6B.zip', 'r') as z:\n","  z.extractall()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TjLJEQ_PwcGi"},"source":["# Load the dataset [5 Marks]\n","\n","1. Using [read_csv()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) in pandas load the given train datasets files **`train_bodies.csv`** and **`train_stances.csv`**\n","\n","2. Using [merge](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) command in pandas merge the two datasets based on the Body ID. \n","\n","Note: Save the final merged dataset in a dataframe with name **`dataset`**."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7gXO1WZ-gFwm","outputId":"932487a1-9532-48ab-8485-9eea4862f098","executionInfo":{"status":"ok","timestamp":1579950116112,"user_tz":-330,"elapsed":21540,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":217}},"source":["import pandas as pd\n","import os\n","train_bodies = os.path.join(project_path, 'train_bodies.csv')\n","train_stances = os.path.join(project_path, 'train_stances.csv')\n","df_tb = pd.read_csv(train_bodies)\n","df_ts = pd.read_csv(train_stances)\n","print(df_tb.head())\n","print(df_ts.head())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   Body ID                                        articleBody\n","0        0  A small meteorite crashed into a wooded area i...\n","1        4  Last week we hinted at what was to come as Ebo...\n","2        5  (NEWSER) – Wonder how long a Quarter Pounder w...\n","3        6  Posting photos of a gun-toting child online, I...\n","4        7  At least 25 suspected Boko Haram insurgents we...\n","                                            Headline  Body ID     Stance\n","0  Police find mass graves with at least '15 bodi...      712  unrelated\n","1  Hundreds of Palestinians flee floods in Gaza a...      158      agree\n","2  Christian Bale passes on role of Steve Jobs, a...      137  unrelated\n","3  HBO and Apple in Talks for $15/Month Apple TV ...     1034  unrelated\n","4  Spider burrowed through tourist's stomach and ...     1923   disagree\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kosAWskdOOT8","outputId":"e7bd8db2-8f0d-4492-bb03-711a000b72ec","executionInfo":{"status":"ok","timestamp":1579950129702,"user_tz":-330,"elapsed":35119,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["df_tb.shape, df_ts.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1683, 2), (49972, 3))"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"KLPTTVOTB-gn","colab_type":"code","outputId":"a1520f79-238a-440a-e7aa-4f0594b7b16e","executionInfo":{"status":"ok","timestamp":1579950129702,"user_tz":-330,"elapsed":35105,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":196}},"source":["df_ts[df_ts['Body ID'] == 158].head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Headline</th>\n","      <th>Body ID</th>\n","      <th>Stance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n","      <td>158</td>\n","      <td>agree</td>\n","    </tr>\n","    <tr>\n","      <th>3107</th>\n","      <td>It's 'rubbish' that Robert Plant turned down £...</td>\n","      <td>158</td>\n","      <td>unrelated</td>\n","    </tr>\n","    <tr>\n","      <th>6392</th>\n","      <td>Robert Plant ripped up $800M Led Zeppelin reun...</td>\n","      <td>158</td>\n","      <td>unrelated</td>\n","    </tr>\n","    <tr>\n","      <th>8059</th>\n","      <td>ISIS Militant “Jihadi John” Identified As Youn...</td>\n","      <td>158</td>\n","      <td>unrelated</td>\n","    </tr>\n","    <tr>\n","      <th>11688</th>\n","      <td>Claim: Comcast Got Complaining Customer Fired ...</td>\n","      <td>158</td>\n","      <td>unrelated</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Headline  Body ID     Stance\n","1      Hundreds of Palestinians flee floods in Gaza a...      158      agree\n","3107   It's 'rubbish' that Robert Plant turned down £...      158  unrelated\n","6392   Robert Plant ripped up $800M Led Zeppelin reun...      158  unrelated\n","8059   ISIS Militant “Jihadi John” Identified As Youn...      158  unrelated\n","11688  Claim: Comcast Got Complaining Customer Fired ...      158  unrelated"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"JD8YQAFcFRoK","colab_type":"code","colab":{}},"source":["dataset = pd.merge(df_tb, df_ts, on='Body ID')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"g4ycQbBCg20S"},"source":["\n","<h2> Check1:</h2>\n","  \n","<h3> You should see the below output if you run `dataset.head()` command as given below </h3>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IUtF7iOmj11k","outputId":"8c3650ad-7cd0-44c6-f662-6617e3ae7ac6","executionInfo":{"status":"ok","timestamp":1579950129703,"user_tz":-330,"elapsed":35085,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":196}},"source":["dataset.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Body ID</th>\n","      <th>articleBody</th>\n","      <th>Headline</th>\n","      <th>Stance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>A small meteorite crashed into a wooded area i...</td>\n","      <td>Soldier shot, Parliament locked down after gun...</td>\n","      <td>unrelated</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>A small meteorite crashed into a wooded area i...</td>\n","      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n","      <td>unrelated</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>A small meteorite crashed into a wooded area i...</td>\n","      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n","      <td>unrelated</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>A small meteorite crashed into a wooded area i...</td>\n","      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n","      <td>unrelated</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>A small meteorite crashed into a wooded area i...</td>\n","      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n","      <td>unrelated</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Body ID  ...     Stance\n","0        0  ...  unrelated\n","1        0  ...  unrelated\n","2        0  ...  unrelated\n","3        0  ...  unrelated\n","4        0  ...  unrelated\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"NMj7i5qQ1Xh_","colab_type":"code","outputId":"57873ff3-4b2f-423c-e830-8301faafba94","executionInfo":{"status":"ok","timestamp":1579950129704,"user_tz":-330,"elapsed":35072,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["dataset.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(49972, 4)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tjzVz2ifijmj"},"source":["## Step2: Data Pre-processing and setting some hyper parameters needed for model\n","\n","\n","#### Run the code given below to set the required parameters.\n","\n","1. `MAX_SENTS` = Maximum no.of sentences to consider in an article.\n","\n","2. `MAX_SENT_LENGTH` = Maximum no.of words to consider in a sentence.\n","\n","3. `MAX_NB_WORDS` = Maximum no.of words in the total vocabualry.\n","\n","4. `MAX_SENTS_HEADING` = Maximum no.of sentences to consider in a heading of an article."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KDXSdpvqjuqw","colab":{}},"source":["MAX_NB_WORDS = 20000\n","MAX_SENTS = 20\n","MAX_SENTS_HEADING = 1\n","MAX_SENT_LENGTH = 20\n","VALIDATION_SPLIT = 0.2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zwE7CPHdiDT-"},"source":["### Download the `Punkt` from nltk using the commands given below. This is for sentence tokenization.\n","\n","For more info on how to use it, read [this](https://stackoverflow.com/questions/35275001/use-of-punktsentencetokenizer-in-nltk).\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lsiKmyJUZ-hU","outputId":"b9b405ea-211a-40ca-bd3d-ade6886c46ae","executionInfo":{"status":"ok","timestamp":1579950130436,"user_tz":-330,"elapsed":35783,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Gqwm_GbwwnhX"},"source":["# Tokenizing the text and loading the pre-trained Glove word embeddings for each token  [5 marks] "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WfZLR24mm32k"},"source":["Keras provides [Tokenizer API](https://keras.io/preprocessing/text/) for preparing text. Read it before going any further."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fLSn9S-5oG4Z"},"source":["#### Import the Tokenizer from keras preprocessing text"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"S-VUgh2yoMlR","outputId":"e0aa30a8-7343-4d66-aa80-be587487d526","executionInfo":{"status":"ok","timestamp":1579950131813,"user_tz":-330,"elapsed":37138,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":79}},"source":["from keras.preprocessing.text import Tokenizer"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eml0Lge4oOuh"},"source":["#### Initialize the Tokenizer class with maximum vocabulary count as `MAX_NB_WORDS` initialized at the start of step2. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Qm85qirPofc2","colab":{}},"source":["toknzr = Tokenizer(num_words=MAX_NB_WORDS)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HBe1KuXDosJ7"},"source":["#### Now, using fit_on_texts() from Tokenizer class, lets encode the data \n","\n","Note: We need to fit articleBody and Headline also to cover all the words."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Q5rk-UyBlmyA","outputId":"08a30c49-2d55-45d8-bc5e-f5247eb09886","executionInfo":{"status":"ok","timestamp":1579950150811,"user_tz":-330,"elapsed":56113,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["txt = dataset['Headline'].append(dataset['articleBody'])\n","print(\"Total documents = \" , len(txt))\n","toknzr.fit_on_texts(txt.values)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Total documents =  99944\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EH8o3_n-wWGa","colab_type":"code","outputId":"61df239b-6e6b-481a-a6fb-d07a8f45463a","executionInfo":{"status":"ok","timestamp":1579950150811,"user_tz":-330,"elapsed":56097,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":100}},"source":["list(toknzr.word_counts.items())[:5]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('soldier', 3582),\n"," ('shot', 10784),\n"," ('parliament', 6992),\n"," ('locked', 211),\n"," ('down', 11448)]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"jnxg65DfxE7w","colab_type":"code","outputId":"30724762-ca7d-4129-edb2-57e45961c502","executionInfo":{"status":"ok","timestamp":1579950150812,"user_tz":-330,"elapsed":56085,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":100}},"source":["list(toknzr.word_docs.items())[:5]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('gunfire', 1560),\n"," ('after', 28779),\n"," ('war', 6241),\n"," ('shot', 5780),\n"," ('memorial', 1782)]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"d1jLFYE6yHyW","colab_type":"code","outputId":"a8640bd9-0e9f-4e9e-ab0c-2cfeb3a59f40","executionInfo":{"status":"ok","timestamp":1579950150812,"user_tz":-330,"elapsed":56076,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["list(toknzr.word_index.items())[0:5]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('the', 1), ('to', 2), ('a', 3), ('of', 4), ('in', 5)]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"y3EOe1agyi8a","colab_type":"code","outputId":"16037432-dc62-46ec-ca10-6692f45af4e4","executionInfo":{"status":"ok","timestamp":1579950150813,"user_tz":-330,"elapsed":56066,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["toknzr.document_count"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["99944"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"omptHX-JpBsN"},"source":["#### fit_on_texts() gives the following attributes in the output as given [here](https://faroit.github.io/keras-docs/1.2.2/preprocessing/text/).\n","\n","* **word_counts:** dictionary mapping words (str) to the number of times they appeared on during fit. Only set after fit_on_texts was called.\n","\n","* **word_docs:** dictionary mapping words (str) to the number of documents/texts they appeared on during fit. Only set after fit_on_texts was called.\n","\n","* **word_index:** dictionary mapping words (str) to their rank/index (int). Only set after fit_on_texts was called.\n","\n","* **document_count:** int. Number of documents (texts/sequences) the tokenizer was trained on. Only set after fit_on_texts or fit_on_sequences was called.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SHnsT2sTtFAA"},"source":["### Now, tokenize the sentences using nltk sent_tokenize() and encode the senteces with the ids we got form the above `t.word_index`\n","\n","Initialise 2 lists with names `texts` and `articles`.\n","\n","```\n","texts = [] to store text of article as it is.\n","\n","articles = [] split the above text into a list of sentences.\n","```"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ctEu-d4c4EZs","colab":{}},"source":["from nltk import sent_tokenize\n","texts = dataset['articleBody'].values\n","articles = [sent_tokenize(t) for t in texts]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"koTVJjoO6P78"},"source":["## Check 2:\n","\n","first element of texts and articles should be as given below. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3mWBW99p5UW9","outputId":"53f45f8c-7075-41c3-d39f-0922403479a0","executionInfo":{"status":"ok","timestamp":1579950185308,"user_tz":-330,"elapsed":90545,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["texts[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'A small meteorite crashed into a wooded area in Nicaragua\\'s capital of Managua overnight, the government said Sunday. Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city\\'s airport, the Associated Press reports. \\n\\nGovernment spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\" House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports. \\nMurillo said Nicaragua will ask international experts to help local scientists in understanding what happened.\\n\\nThe crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee. He said it is still not clear if the meteorite disintegrated or was buried.\\n\\nHumberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.\\n\\n\"We have to study it more because it could be ice or rock,\" he said.\\n\\nWilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light. We have to ask if anyone has a photo or something.\"\\n\\nLocal residents reported hearing a loud boom Saturday night, but said they didn\\'t see anything strange in the sky.\\n\\n\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast. We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.\\n\\nThe site of the crater is near Managua\\'s international airport and an air force base. Only journalists from state media were allowed to visit it.'"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WtIjO3ht5EKA","outputId":"106e34dc-4b75-45aa-befa-dc4594a4d21d","executionInfo":{"status":"ok","timestamp":1579950185309,"user_tz":-330,"elapsed":90537,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":304}},"source":["articles[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"A small meteorite crashed into a wooded area in Nicaragua's capital of Managua overnight, the government said Sunday.\",\n"," \"Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city's airport, the Associated Press reports.\",\n"," 'Government spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\"',\n"," 'House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports.',\n"," 'Murillo said Nicaragua will ask international experts to help local scientists in understanding what happened.',\n"," 'The crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee.',\n"," 'He said it is still not clear if the meteorite disintegrated or was buried.',\n"," 'Humberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.',\n"," '\"We have to study it more because it could be ice or rock,\" he said.',\n"," 'Wilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light.',\n"," 'We have to ask if anyone has a photo or something.\"',\n"," \"Local residents reported hearing a loud boom Saturday night, but said they didn't see anything strange in the sky.\",\n"," '\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast.',\n"," 'We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.',\n"," \"The site of the crater is near Managua's international airport and an air force base.\",\n"," 'Only journalists from state media were allowed to visit it.']"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"2spE4zB1xbxv","colab_type":"code","outputId":"ce9fa7f4-f69d-42e7-a79a-345472829b8b","executionInfo":{"status":"ok","timestamp":1579950185309,"user_tz":-330,"elapsed":90528,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["len(articles)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["49972"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fpuRIA7cCfcY"},"source":["# Now iterate through each article and each sentence to encode the words into ids using t.word_index  [5 marks] \n","\n","Here, to get words from sentence you can use `text_to_word_sequence` from keras preprocessing text.\n","\n","1. Import text_to_word_sequence\n","\n","2. Initialize a variable of shape (no.of articles, MAX_SENTS, MAX_SENT_LENGTH) with name `data` with zeros first (you can use numpy [np.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html) to initialize with all zeros)and then update it while iterating through the words and sentences in each article."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YVyClBULCqWj","outputId":"d6ab02cd-ab6b-4685-e077-3c5fdc464bf5","executionInfo":{"status":"ok","timestamp":1579950203165,"user_tz":-330,"elapsed":108376,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["import numpy as np\n","from keras.preprocessing.text import text_to_word_sequence\n","NUM_ARTICLES = len(articles)\n","data = np.zeros(shape=(NUM_ARTICLES, MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n","\n","article_counter = 0\n","for article in articles:\n","  sent_counter = 0\n","  #print(\"Sentences in article = \", len(article))\n","  if (len(article) > MAX_SENTS):\n","    article = article[0:MAX_SENTS]\n","  for sent in article:\n","      vals = np.array([toknzr.word_index[word] for word in text_to_word_sequence(sent)])\n","      if vals.shape[0] > MAX_SENT_LENGTH:\n","        vals = vals[0:MAX_SENT_LENGTH]\n","      data[article_counter, sent_counter, 0:vals.shape[0]] = vals\n","      sent_counter += 1\n","  article_counter += 1\n","print('Done')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PNhpQxlvuZHr","colab_type":"code","outputId":"0dd8bb92-bfe3-4ef9-dc95-0f862c5d6d3c","executionInfo":{"status":"ok","timestamp":1579950203166,"user_tz":-330,"elapsed":108368,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["data.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(49972, 20, 20)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bFdmiDYcE144"},"source":["### Check 3:\n","\n","Accessing first element in data should give something like given below."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TsFWW5C2Djog","outputId":"9e490068-6b48-41cd-c9f8-a34819b20d5c","executionInfo":{"status":"ok","timestamp":1579950203167,"user_tz":-330,"elapsed":108361,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["data[0, :, :]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[    3,   481,   427,  7211,    81,     3,  3734,   331,     5,\n","         3892,   350,     4,  1431,  2960,     1,    89,    12,   466,\n","            0,     0],\n","       [  758,    95,  1047,     3,  2679,  1752,     7,   189,     3,\n","         1217,  1075,  2030,   700,   159,     1,  3033,   448,     1,\n","          555,   235],\n","       [   89,  1068,  4117,  2349,    12,     3,  1092,  3307,    19,\n","            1,    89,     2,  1793,     1,   521,  2009,    15,     9,\n","            3,  3111],\n","       [  181,  3641,   972,   200,  2558,    44,  6776,  1722,  1252,\n","            5, 13324, 17943,     1,   778,    31,   740,  3991,    67,\n","           85,     0],\n","       [ 2349,    12,  1557,    38,  1094,   351,   775,     2,   367,\n","          260,  1770,     5,  4455,    70,   494,     0,     0,     0,\n","            0,     0],\n","       [    1,   700,   189,    19,     1,   427,    32,     3,  7423,\n","            4,  2159,  1252,     6,     3,  5271,     4,  1217,  1252,\n","           12,  3365],\n","       [   13,    12,    15,     8,   149,    25,   543,    64,     1,\n","          427,  3727,    41,     9,  1850,     0,     0,     0,     0,\n","            0,     0],\n","       [ 3365,  5734,     4,     1,  5876,   614,    21,     1,   311,\n","         3439,   795,     4,  1557,    12,     1,   427,    69,    23,\n","          787,     2],\n","       [   37,    17,     2,  1793,    15,    52,   120,    15,    69,\n","           23,  4923,    41,  1963,    13,    12,     0,     0,     0,\n","            0,     0],\n","       [ 4737,  3339,    24,  3971,     2,     1,  1316,     4,  3073,\n","         1655,    12,    15,     9,   195,  1421,     7,    58,    40,\n","           95,     3],\n","       [   37,    17,     2,  1094,    64,   510,    20,     3,   250,\n","           41,   264,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [  260,   758,    95,  1047,     3,  1808,  1752,   531,   276,\n","           29,    12,    33,   703,   163,   893,  1421,     5,     1,\n","         2081,     0],\n","       [   35,     9,  2058,    10,   116,  5828,     6,    35,   576,\n","          656,   104,    59,     4,     3,  2411,    35,   241,     3,\n","          512,  1911],\n","       [   37,   341,    15,     9,     3,  2082,   120,    37,   881,\n","           24,  4456,  2585,  4317,  4924,    55,     1,   555,   235,\n","            0,     0],\n","       [    1,   255,     4,     1,   700,     8,   159,  3961,   351,\n","          448,     6,    24,   155,   465,  1930,     0,     0,     0,\n","            0,     0],\n","       [  126,   921,    22,    47,   100,    36,  1834,     2,  1213,\n","           15,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hTG6JySHehkT"},"source":["# Repeat the same process for the `Headings` as well. Use variables with names `texts_heading` and `articles_heading` accordingly. [5 marks] "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_CliiIhLemJV","outputId":"ef02c7bf-cea8-4d69-9840-effaaeeea9c4","executionInfo":{"status":"ok","timestamp":1579950205385,"user_tz":-330,"elapsed":110569,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["texts_heading = dataset['Headline'].values\n","articles_heading = [sent_tokenize(t) for t in texts_heading]\n","\n","NUM_HEADLINES = len(articles_heading)\n","data_heading = np.zeros(shape=(NUM_HEADLINES, MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n","\n","article_counter = 0\n","for article in articles_heading:\n","  sent_counter = 0\n","  #print(\"Sentences in article = \", len(article))\n","  if (len(article) > MAX_SENTS):\n","    article = article[0:MAX_SENTS]\n","  for sent in article:\n","      vals = np.array([toknzr.word_index[word] for word in text_to_word_sequence(sent)])\n","      if vals.shape[0] > MAX_SENT_LENGTH:\n","        vals = vals[0:MAX_SENT_LENGTH]\n","      data_heading[article_counter, sent_counter, 0:vals.shape[0]] = vals\n","      sent_counter += 1\n","  article_counter += 1\n","print('Done')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZMNTosn2pGj0","colab_type":"code","outputId":"1793d6c7-8258-4e8f-f75e-f387d9046101","executionInfo":{"status":"ok","timestamp":1579950205386,"user_tz":-330,"elapsed":110561,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["data_heading[0,:,:]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  717,   206,   343,  7118,   193,    34,  1338, 11495,    21,\n","          233,   686,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0],\n","       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iaH0Ey1qe_Co"},"source":["### Now the features are ready, lets make the labels ready for the model to process.\n","\n","### Convert labels into one-hot vectors\n","\n","You can use [get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) in pandas to create one-hot vectors."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Zq-VcgM8fat1","outputId":"ce1bf314-04cd-4aad-88e6-63bed8c2726a","executionInfo":{"status":"ok","timestamp":1579950205388,"user_tz":-330,"elapsed":110555,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["labels = pd.get_dummies(dataset['Stance'])\n","labels.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(49972, 4)"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"40mA8FI2fcxZ"},"source":["### Check 4:\n","\n","The shape of data and labels shoould match the given below numbers."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vpEWEnjFfnFR","outputId":"04ce01c9-a7c5-43e5-f5db-3727dc22fe4e","executionInfo":{"status":"ok","timestamp":1579950205388,"user_tz":-330,"elapsed":110546,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["print('Shape of data tensor:', data.shape)\n","print('Shape of label tensor:', labels.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Shape of data tensor: (49972, 20, 20)\n","Shape of label tensor: (49972, 4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sDOxHdR3frDu"},"source":["### Shuffle the data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-Ra-yYTvfzRt","colab":{}},"source":["## get numbers upto no.of articles\n","indices = np.arange(data.shape[0])\n","## shuffle the numbers\n","np.random.shuffle(indices)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMBVuSc41RD6","colab_type":"code","outputId":"0f1b0187-bcd9-4e7b-a592-3bbcb6dacaff","executionInfo":{"status":"ok","timestamp":1579950205389,"user_tz":-330,"elapsed":110533,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":137}},"source":["labels.iloc[[0, 4, 5]]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>agree</th>\n","      <th>disagree</th>\n","      <th>discuss</th>\n","      <th>unrelated</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   agree  disagree  discuss  unrelated\n","0      0         0        0          1\n","4      0         0        0          1\n","5      0         0        0          1"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LKnSqwIFf3Iy","colab":{}},"source":["## shuffle the data\n","data = data[indices]\n","data_heading = data_heading[indices]\n","## shuffle the labels according to data\n","labels = labels.iloc[indices,:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VonVf4fj2pIU","colab_type":"code","outputId":"20a7c51e-4ebe-49ff-9b63-7eae535ea1e3","executionInfo":{"status":"ok","timestamp":1579950205390,"user_tz":-330,"elapsed":110519,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":196}},"source":["labels[0:5]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>agree</th>\n","      <th>disagree</th>\n","      <th>discuss</th>\n","      <th>unrelated</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>30800</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19991</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16674</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>47197</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16301</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       agree  disagree  discuss  unrelated\n","30800      0         0        0          1\n","19991      0         0        0          1\n","16674      0         0        0          1\n","47197      0         0        0          1\n","16301      0         0        0          1"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JcOFVfPBf9kA"},"source":["### Split into train and validation sets. Split the train set 80:20 ratio to get the train and validation sets.\n","\n","\n","Use the variable names as given below:\n","\n","x_train, x_val - for body of articles.\n","\n","x-heading_train, x_heading_val - for heading of articles.\n","\n","y_train - for training labels.\n","\n","y_val - for validation labels.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2neh9Wcof8iR","colab":{}},"source":["idx_80_perc = round(len(data) * 80 / 100)\n","x_train = data[0:idx_80_perc]\n","x_val = data[idx_80_perc:]\n","\n","y_train = labels[0:idx_80_perc]\n","y_val = labels[idx_80_perc:]\n","\n","idx_80_perc = round(len(data_heading) * 80 / 100)\n","x_heading_train = data[0:idx_80_perc]\n","x_heading_val = data[idx_80_perc:] "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UTyvoHrsgMDw"},"source":["### Check 5:\n","\n","The shape of x_train, x_val, y_train and y_val should match the below numbers."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KLEbiw2Yghe2","outputId":"53741770-d271-46d2-8765-334ebe409fc9","executionInfo":{"status":"ok","timestamp":1579950205390,"user_tz":-330,"elapsed":110505,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":117}},"source":["print(x_train.shape)\n","print(y_train.shape)\n","\n","print(x_val.shape)\n","print(y_val.shape)\n","\n","print(x_heading_train.shape)\n","print(x_heading_val.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(39978, 20, 20)\n","(39978, 4)\n","(9994, 20, 20)\n","(9994, 4)\n","(39978, 20, 20)\n","(9994, 20, 20)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2LHkcQwR7y4s","colab_type":"code","colab":{}},"source":["# Earlier tried to initialize vocab_size with num_words, but vocab_size is not really controlled by toknzr.num_word\n","# vocab_size = toknzr.num_words\n","# Hence changed vocab_size to below\n","vocab_size = len(toknzr.word_index.items())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yNnoBtArhJ1E"},"source":["### Create embedding matrix with the glove embeddings\n","\n","\n","Run the below code to create embedding_matrix which has all the words and their glove embedding if present in glove word list."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eKqn2IL2ZF8v","outputId":"d7a905de-52b7-42e8-c737-733495aa7b97","executionInfo":{"status":"ok","timestamp":1579950217272,"user_tz":-330,"elapsed":122367,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# load the whole embedding into memory\n","embeddings_index = dict()\n","f = open('./glove.6B.100d.txt')\n","for line in f:\n","\tvalues = line.split()\n","\tword = values[0]\n","\tcoefs = np.asarray(values[1:], dtype='float32')\n","\tembeddings_index[word] = coefs\n","f.close()\n","print('Loaded %s word vectors.' % len(embeddings_index))\n","\n","# create a weight matrix for words in training docs\n","embedding_matrix = np.zeros((vocab_size, 100))\n","\n","embedding_vector_missing = []\n","for word, i in toknzr.word_index.items():\n","\tembedding_vector = embeddings_index.get(word)\n","\tif embedding_vector is not None:\n","\t\tembedding_matrix[i] = embedding_vector\n","\telse:\n","\t\tembedding_vector_missing.append(word)\n","\t\n","print(f'Embedding Vector missing for total {len(embedding_vector_missing)} words.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded 400000 word vectors.\n","Embedding Vector missing for total 6781 words.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LRi4o3ZspDFU"},"source":["# Try the sequential model approach and report the accuracy score. [10 marks]  "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zSZDnPWkw2ZZ"},"source":["### Import layers from Keras to build the model"]},{"cell_type":"code","metadata":{"id":"JW1Jt_UL9JSr","colab_type":"code","outputId":"43f2e035-e171-4b0e-c1e8-b5d132e1d05f","executionInfo":{"status":"ok","timestamp":1579950217273,"user_tz":-330,"elapsed":122353,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["x = x_train\n","x_train = np.reshape(x, (x.shape[0], (x.shape[1] * x.shape[2])))\n","x = x_val\n","x_val = np.reshape(x, (x.shape[0], (x.shape[1] * x.shape[2])))\n","\n","print(x_train.shape)\n","print(x_val.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(39978, 400)\n","(9994, 400)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WKONqdq1JNCM","colab_type":"code","colab":{}},"source":["x = x_heading_train\n","x_heading_train = np.reshape(x, (x.shape[0], (x.shape[1] * x.shape[2])))\n","\n","x = x_heading_val\n","x_heading_val = np.reshape(x, (x.shape[0], (x.shape[1] * x.shape[2])))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KeNXo7LSJ2Dj","colab_type":"code","outputId":"84a6031a-be78-49bf-b1c2-9dfc6b452042","executionInfo":{"status":"ok","timestamp":1579950217274,"user_tz":-330,"elapsed":122341,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["print(x_heading_train.shape)\n","print(x_heading_val.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(39978, 400)\n","(9994, 400)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KtxQ_NbuLBua","colab_type":"code","colab":{}},"source":["x_t = np.hstack((x_train, x_heading_train))\n","x_v = np.hstack((x_val, x_heading_val))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2-jJRN1oLcPD","colab_type":"code","outputId":"a9eac5fe-5d53-486c-e405-3d0831b2f1f1","executionInfo":{"status":"ok","timestamp":1579950217275,"user_tz":-330,"elapsed":122327,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["print(x_t.shape)\n","print(x_v.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(39978, 800)\n","(9994, 800)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5AgwQsfMrzAQ","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Embedding\n","from keras.layers import LSTM\n","from keras.layers import Bidirectional\n","from keras.callbacks import ReduceLROnPlateau"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P95cgEknWtSx","colab_type":"text"},"source":["### Model using Simple LSTM"]},{"cell_type":"code","metadata":{"id":"3N-9ApxD0J--","colab_type":"code","outputId":"626b0a9a-ab23-417f-b65a-ce48d867ede7","executionInfo":{"status":"ok","timestamp":1579953635862,"user_tz":-330,"elapsed":1263655,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# define model\n","model = Sequential()\n","e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=x_t.shape[1], trainable=False)\n","model.add(e)\n","model.add(LSTM(100, dropout=0.1, recurrent_dropout=0.1))\n","#model.add(Flatten())\n","model.add(Dense(4, activation='softmax'))\n","# compile the model\n","lr_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.1, epsilon=1e-5, patience=10, verbose=1)\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n","# summarize the model\n","print(model.summary())\n","\n","# fit the model\n","model.fit(x_t, y_train, validation_data=(x_v, y_val), epochs=50, verbose=1, callbacks=[lr_reduce], batch_size=1024)\n","# evaluate the model\n","loss, accuracy = model.evaluate(x_v, y_val, verbose=0)\n","print('Accuracy: %f' % (accuracy*100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 800, 100)          2787300   \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 100)               80400     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 4)                 404       \n","=================================================================\n","Total params: 2,868,104\n","Trainable params: 80,804\n","Non-trainable params: 2,787,300\n","_________________________________________________________________\n","None\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1335: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n","  warnings.warn('`epsilon` argument is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","Train on 39978 samples, validate on 9994 samples\n","Epoch 1/50\n","39978/39978 [==============================] - 66s 2ms/step - loss: 0.8514 - acc: 0.7201 - val_loss: 0.8452 - val_acc: 0.7288\n","Epoch 2/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7886 - acc: 0.7330 - val_loss: 0.7979 - val_acc: 0.7291\n","Epoch 3/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7813 - acc: 0.7341 - val_loss: 0.8872 - val_acc: 0.7250\n","Epoch 4/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7777 - acc: 0.7346 - val_loss: 0.8153 - val_acc: 0.7294\n","Epoch 5/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7702 - acc: 0.7364 - val_loss: 0.7820 - val_acc: 0.7316\n","Epoch 6/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7655 - acc: 0.7382 - val_loss: 0.7787 - val_acc: 0.7306\n","Epoch 7/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7620 - acc: 0.7392 - val_loss: 0.7963 - val_acc: 0.7350\n","Epoch 8/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7585 - acc: 0.7402 - val_loss: 0.7824 - val_acc: 0.7334\n","Epoch 9/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7542 - acc: 0.7419 - val_loss: 0.7632 - val_acc: 0.7370\n","Epoch 10/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7517 - acc: 0.7426 - val_loss: 0.7654 - val_acc: 0.7347\n","Epoch 11/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7485 - acc: 0.7429 - val_loss: 0.7800 - val_acc: 0.7358\n","Epoch 12/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7456 - acc: 0.7431 - val_loss: 0.7582 - val_acc: 0.7386\n","Epoch 13/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7420 - acc: 0.7448 - val_loss: 0.7560 - val_acc: 0.7379\n","Epoch 14/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7397 - acc: 0.7456 - val_loss: 0.7552 - val_acc: 0.7364\n","Epoch 15/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7370 - acc: 0.7459 - val_loss: 0.7461 - val_acc: 0.7403\n","Epoch 16/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7342 - acc: 0.7453 - val_loss: 0.7748 - val_acc: 0.7399\n","Epoch 17/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7332 - acc: 0.7469 - val_loss: 0.7459 - val_acc: 0.7406\n","Epoch 18/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7297 - acc: 0.7479 - val_loss: 0.7567 - val_acc: 0.7399\n","Epoch 19/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7357 - acc: 0.7472 - val_loss: 0.7546 - val_acc: 0.7405\n","Epoch 20/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7313 - acc: 0.7479 - val_loss: 0.7473 - val_acc: 0.7404\n","Epoch 21/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7341 - acc: 0.7478 - val_loss: 0.7539 - val_acc: 0.7435\n","Epoch 22/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7323 - acc: 0.7498 - val_loss: 0.7609 - val_acc: 0.7393\n","Epoch 23/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7293 - acc: 0.7501 - val_loss: 0.7697 - val_acc: 0.7274\n","Epoch 24/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7260 - acc: 0.7494 - val_loss: 0.7519 - val_acc: 0.7412\n","Epoch 25/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7258 - acc: 0.7509 - val_loss: 0.7447 - val_acc: 0.7424\n","Epoch 26/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7218 - acc: 0.7504 - val_loss: 0.7479 - val_acc: 0.7345\n","Epoch 27/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7228 - acc: 0.7519 - val_loss: 0.7867 - val_acc: 0.7434\n","Epoch 28/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7231 - acc: 0.7525 - val_loss: 0.7382 - val_acc: 0.7466\n","Epoch 29/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7210 - acc: 0.7529 - val_loss: 0.7504 - val_acc: 0.7458\n","Epoch 30/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7149 - acc: 0.7540 - val_loss: 0.7215 - val_acc: 0.7464\n","Epoch 31/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7141 - acc: 0.7529 - val_loss: 0.7384 - val_acc: 0.7440\n","Epoch 32/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7151 - acc: 0.7539 - val_loss: 0.7383 - val_acc: 0.7443\n","Epoch 33/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7137 - acc: 0.7544 - val_loss: 0.7375 - val_acc: 0.7453\n","Epoch 34/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7107 - acc: 0.7551 - val_loss: 0.7194 - val_acc: 0.7459\n","Epoch 35/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7059 - acc: 0.7552 - val_loss: 0.7253 - val_acc: 0.7468\n","Epoch 36/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7058 - acc: 0.7552 - val_loss: 0.7113 - val_acc: 0.7466\n","Epoch 37/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7042 - acc: 0.7569 - val_loss: 0.7117 - val_acc: 0.7478\n","Epoch 38/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7020 - acc: 0.7563 - val_loss: 0.7313 - val_acc: 0.7457\n","Epoch 39/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.7000 - acc: 0.7563 - val_loss: 0.7091 - val_acc: 0.7498\n","Epoch 40/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.6975 - acc: 0.7574 - val_loss: 0.7202 - val_acc: 0.7479\n","Epoch 41/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.6961 - acc: 0.7571 - val_loss: 0.7060 - val_acc: 0.7497\n","Epoch 42/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.6958 - acc: 0.7564 - val_loss: 0.7047 - val_acc: 0.7466\n","Epoch 43/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.6912 - acc: 0.7570 - val_loss: 0.7296 - val_acc: 0.7421\n","Epoch 44/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.6937 - acc: 0.7562 - val_loss: 0.7782 - val_acc: 0.7468\n","Epoch 45/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.6882 - acc: 0.7569 - val_loss: 0.6850 - val_acc: 0.7506\n","Epoch 46/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.6812 - acc: 0.7586 - val_loss: 0.6932 - val_acc: 0.7506\n","Epoch 47/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.6780 - acc: 0.7587 - val_loss: 0.6895 - val_acc: 0.7508\n","Epoch 48/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.6735 - acc: 0.7616 - val_loss: 0.7013 - val_acc: 0.7510\n","Epoch 49/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.6702 - acc: 0.7617 - val_loss: 0.6906 - val_acc: 0.7456\n","Epoch 50/50\n","39978/39978 [==============================] - 65s 2ms/step - loss: 0.6664 - acc: 0.7628 - val_loss: 0.7424 - val_acc: 0.7149\n","Accuracy: 71.492896\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gpkVhIbx3gr1"},"source":["### Model using Bidirectional LSTM"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"G_8QXh-rmPFq","outputId":"eb122f01-643c-4c90-a53a-3802fbf9b32b","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1579962754832,"user_tz":-330,"elapsed":3031150,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}}},"source":["# define model\n","model = Sequential()\n","e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=x_t.shape[1], trainable=False)\n","model.add(e)\n","model.add(Bidirectional(LSTM(128, dropout=0.1, recurrent_dropout=0.1)))\n","model.add(Dense(4, activation='softmax'))\n","# compile the model\n","lr_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.1, epsilon=1e-5, patience=10, verbose=1)\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n","# summarize the model\n","print(model.summary())\n","\n","# fit the model\n","model.fit(x_t, y_train, validation_data=(x_v, y_val), epochs=50, verbose=1, callbacks=[lr_reduce], batch_size=1024)\n","# evaluate the model\n","loss, accuracy = model.evaluate(x_v, y_val, verbose=0)\n","print('Accuracy: %f' % (accuracy*100))"],"execution_count":45,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1335: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n","  warnings.warn('`epsilon` argument is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 800, 100)          2787300   \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 256)               234496    \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 4)                 1028      \n","=================================================================\n","Total params: 3,022,824\n","Trainable params: 235,524\n","Non-trainable params: 2,787,300\n","_________________________________________________________________\n","None\n","Train on 39978 samples, validate on 9994 samples\n","Epoch 1/50\n","39978/39978 [==============================] - 185s 5ms/step - loss: 0.7996 - acc: 0.7208 - val_loss: 0.8856 - val_acc: 0.7269\n","Epoch 2/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.7510 - acc: 0.7337 - val_loss: 0.7680 - val_acc: 0.7370\n","Epoch 3/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.7254 - acc: 0.7391 - val_loss: 0.7323 - val_acc: 0.7317\n","Epoch 4/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.7062 - acc: 0.7451 - val_loss: 0.7390 - val_acc: 0.7280\n","Epoch 5/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.6901 - acc: 0.7510 - val_loss: 0.6957 - val_acc: 0.7482\n","Epoch 6/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.6707 - acc: 0.7554 - val_loss: 0.6840 - val_acc: 0.7487\n","Epoch 7/50\n","39978/39978 [==============================] - 176s 4ms/step - loss: 0.6565 - acc: 0.7599 - val_loss: 0.6746 - val_acc: 0.7542\n","Epoch 8/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.6401 - acc: 0.7624 - val_loss: 0.6755 - val_acc: 0.7481\n","Epoch 9/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.6237 - acc: 0.7701 - val_loss: 0.6443 - val_acc: 0.7642\n","Epoch 10/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.6130 - acc: 0.7719 - val_loss: 0.6330 - val_acc: 0.7684\n","Epoch 11/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.5978 - acc: 0.7769 - val_loss: 0.6537 - val_acc: 0.7618\n","Epoch 12/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5921 - acc: 0.7801 - val_loss: 0.6483 - val_acc: 0.7649\n","Epoch 13/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5807 - acc: 0.7826 - val_loss: 0.6452 - val_acc: 0.7540\n","Epoch 14/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5741 - acc: 0.7859 - val_loss: 0.5989 - val_acc: 0.7680\n","Epoch 15/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5663 - acc: 0.7877 - val_loss: 0.5860 - val_acc: 0.7826\n","Epoch 16/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.5565 - acc: 0.7909 - val_loss: 0.6823 - val_acc: 0.7152\n","Epoch 17/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5526 - acc: 0.7929 - val_loss: 0.5833 - val_acc: 0.7835\n","Epoch 18/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.5445 - acc: 0.7968 - val_loss: 0.5872 - val_acc: 0.7850\n","Epoch 19/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5402 - acc: 0.7965 - val_loss: 0.5701 - val_acc: 0.7894\n","Epoch 20/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5357 - acc: 0.7985 - val_loss: 0.5809 - val_acc: 0.7843\n","Epoch 21/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5321 - acc: 0.7999 - val_loss: 0.5791 - val_acc: 0.7938\n","Epoch 22/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5282 - acc: 0.8016 - val_loss: 0.5778 - val_acc: 0.7923\n","Epoch 23/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5231 - acc: 0.8025 - val_loss: 0.5696 - val_acc: 0.7971\n","Epoch 24/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5200 - acc: 0.8055 - val_loss: 0.5586 - val_acc: 0.8013\n","Epoch 25/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5172 - acc: 0.8066 - val_loss: 0.5538 - val_acc: 0.7928\n","Epoch 26/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5130 - acc: 0.8063 - val_loss: 0.5682 - val_acc: 0.7982\n","Epoch 27/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5119 - acc: 0.8066 - val_loss: 0.5381 - val_acc: 0.7964\n","Epoch 28/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5081 - acc: 0.8075 - val_loss: 0.6013 - val_acc: 0.7962\n","Epoch 29/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5068 - acc: 0.8074 - val_loss: 0.5417 - val_acc: 0.7902\n","Epoch 30/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5057 - acc: 0.8098 - val_loss: 0.5261 - val_acc: 0.8034\n","Epoch 31/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.4994 - acc: 0.8104 - val_loss: 0.5463 - val_acc: 0.7932\n","Epoch 32/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.5019 - acc: 0.8093 - val_loss: 0.5427 - val_acc: 0.7857\n","Epoch 33/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.4968 - acc: 0.8117 - val_loss: 0.5394 - val_acc: 0.8061\n","Epoch 34/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.4958 - acc: 0.8118 - val_loss: 0.5498 - val_acc: 0.8052\n","Epoch 35/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.4958 - acc: 0.8114 - val_loss: 0.5293 - val_acc: 0.8022\n","Epoch 36/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.4937 - acc: 0.8133 - val_loss: 0.5258 - val_acc: 0.8044\n","Epoch 37/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.4896 - acc: 0.8127 - val_loss: 0.5257 - val_acc: 0.8010\n","Epoch 38/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.4901 - acc: 0.8135 - val_loss: 0.5226 - val_acc: 0.8008\n","Epoch 39/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.4877 - acc: 0.8136 - val_loss: 0.5243 - val_acc: 0.8083\n","Epoch 40/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.4896 - acc: 0.8139 - val_loss: 0.5211 - val_acc: 0.8068\n","Epoch 41/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.4859 - acc: 0.8140 - val_loss: 0.5273 - val_acc: 0.7967\n","Epoch 42/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.4851 - acc: 0.8155 - val_loss: 0.5203 - val_acc: 0.8090\n","Epoch 43/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.4818 - acc: 0.8160 - val_loss: 0.5177 - val_acc: 0.8083\n","Epoch 44/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.4820 - acc: 0.8150 - val_loss: 0.5325 - val_acc: 0.8060\n","Epoch 45/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.4811 - acc: 0.8159 - val_loss: 0.5258 - val_acc: 0.7982\n","Epoch 46/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.4820 - acc: 0.8139 - val_loss: 0.5209 - val_acc: 0.7980\n","Epoch 47/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.4813 - acc: 0.8161 - val_loss: 0.5272 - val_acc: 0.7974\n","Epoch 48/50\n","39978/39978 [==============================] - 175s 4ms/step - loss: 0.4774 - acc: 0.8166 - val_loss: 0.5292 - val_acc: 0.7940\n","Epoch 49/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.4779 - acc: 0.8143 - val_loss: 0.5184 - val_acc: 0.8121\n","Epoch 50/50\n","39978/39978 [==============================] - 174s 4ms/step - loss: 0.4775 - acc: 0.8169 - val_loss: 0.5261 - val_acc: 0.7962\n","Accuracy: 79.617771\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cY7chh-wLuS2","colab_type":"text"},"source":["    So it appears that Bidirectional LSTM gives way better results than Simple LSTM"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"R47A6Ysfev3l"},"source":["## Build the same model with attention layers included for better performance (Optional)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wivJ-eVkfEOm","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"olqo5ytRe7eq"},"source":["## Fit the model and report the accuracy score for the model with attention layer (Optional)"]},{"cell_type":"markdown","metadata":{"id":"YCJsLU1t3l_k","colab_type":"text"},"source":["# Extra - Understanding Keras Embedding Layer\n","\n","## How Keras Embedding Layer can be used to create word embedding. \n","A word embedding is a class of approaches for representing words and documents using a dense vector representation.\n","\n","It is an improvement over more the traditional bag-of-word model encoding schemes where large sparse vectors were used to represent each word or to score each word within a vector to represent an entire vocabulary. These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values.\n","\n","Instead, in an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.\n","\n","The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n","\n","The position of a word in the learned vector space is referred to as its embedding.\n","\n","Two popular examples of methods of learning word embeddings from text include:\n","\n","Word2Vec.\n","GloVe.\n","In addition to these carefully designed methods, a word embedding can be learned as part of a deep learning model. This can be a slower approach, but tailors the model to a specific training dataset.\n","\n","[Ref](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1zgxPrhzfBkv","colab":{"base_uri":"https://localhost:8080/","height":468},"outputId":"1845e2af-b822-468e-81c6-028bff34ee16","executionInfo":{"status":"ok","timestamp":1579962756148,"user_tz":-330,"elapsed":1322,"user":{"displayName":"Beejal Vibhakar","photoUrl":"","userId":"07210044716704128812"}}},"source":["from numpy import array\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers.embeddings import Embedding\n","# define documents\n","docs = ['Well done!',\n","\t\t'Good work',\n","\t\t'Great effort',\n","\t\t'nice work',\n","\t\t'Excellent!',\n","\t\t'Weak',\n","\t\t'Poor effort!',\n","\t\t'not good',\n","\t\t'poor work',\n","\t\t'Could have done better.']\n","# define class labels\n","labels = array([1,1,1,1,1,0,0,0,0,0])\n","# integer encode the documents\n","vocab_size = 50\n","encoded_docs = [one_hot(d, vocab_size) for d in docs]\n","print(encoded_docs)\n","# pad documents to a max length of 4 words\n","max_length = 4\n","padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","print(padded_docs)\n","# define the model\n","model = Sequential()\n","model.add(Embedding(vocab_size, 8, input_length=max_length))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","# compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","# summarize the model\n","print(model.summary())\n","# fit the model\n","model.fit(padded_docs, labels, epochs=50, verbose=0)\n","# evaluate the model\n","loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n","print('Accuracy: %f' % (accuracy*100))"],"execution_count":46,"outputs":[{"output_type":"stream","text":["[[19, 19], [23, 43], [46, 16], [21, 43], [33], [6], [19, 16], [22, 23], [19, 43], [8, 46, 19, 45]]\n","[[19 19  0  0]\n"," [23 43  0  0]\n"," [46 16  0  0]\n"," [21 43  0  0]\n"," [33  0  0  0]\n"," [ 6  0  0  0]\n"," [19 16  0  0]\n"," [22 23  0  0]\n"," [19 43  0  0]\n"," [ 8 46 19 45]]\n","Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, 4, 8)              400       \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 32)                0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 33        \n","=================================================================\n","Total params: 433\n","Trainable params: 433\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Accuracy: 89.999998\n"],"name":"stdout"}]}]}